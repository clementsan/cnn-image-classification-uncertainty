{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  fastai-v1  inference on Test data - experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup CUDA_VISIBLE DEVICES for titan.sci.utah.edu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries - fastai_v1\n",
    "\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O and hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and hyper-parameters\n",
    "\n",
    "# CSV file contains test dataset only (synthetic data)\n",
    "csv_test_FileName = 'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_test.csv'\n",
    "csv_test = os.path.join('../CSV_InputFiles_TargetClass',csv_test_FileName)\n",
    "\n",
    "csv_result = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_test-Prediction.csv')\n",
    "\n",
    "csv_result_MajVoting = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_test-Prediction_MajVoting.csv')\n",
    "\n",
    "# Network\n",
    "model_path = os.path.join(os.getcwd(),'models')\n",
    "model_file = ('TargetClass_fastai-v1_224_all_resnet50.pkl')\n",
    "\n",
    "# Network architecture\n",
    "arch = models.resnet50\n",
    "# Image size\n",
    "sz = 224\n",
    "# Batch size\n",
    "bs = 32\n",
    "# Default learning rate\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file and create dataframe\n",
    "df_test = pd.read_csv(csv_test, sep=',')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby(['Label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_size = df_test.groupby(['Label']).size()\n",
    "#df_test_size = df_test_size.reindex(classes_Labels_ordered)\n",
    "df_test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bar graph\n",
    "# pd.value_counts(df_test['Label']).sort_index().plot(kind='bar', title = 'Starting Material - test dataset')\n",
    "# fig1 = plt.gcf()\n",
    "# plt.tight_layout()\n",
    "# fig1.savefig('BarGraph_Distribution_StartingMaterial_TestData.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns_plot = sns.countplot(x=\"Label\", data=df_test)\n",
    "sns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig = sns_plot.get_figure()\n",
    "fig.savefig(\"BarGraph_Distribution_TargetClass_TestData.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - Test dataset - without TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ImageList.from_csv(os.getcwd(), csv_test_FileName, folder='../Data_TargetClass')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main commands to load data and model\n",
    "learn = load_learner(model_path,model_file, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test, _, losses = learn.get_preds(ds_type=DatasetType.Test,with_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_classes = [learn.data.classes[np.argmax(pred)] for pred in y_pred_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_test[0])\n",
    "print(y_pred_test[0].numpy())\n",
    "print(np.sum(y_pred_test[0].numpy()))\n",
    "print(np.argmax(y_pred_test[0]))\n",
    "print(y_pred_test_classes[0])\n",
    "print(losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_test_classes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FileNames = [i.split('/', -1)[-1] for i in learn.data.test_ds.items]\n",
    "FileNames = ['/'.join(i.split('/', -1)[-4:]) for i in learn.data.test_ds.items]\n",
    "print(FileNames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for prediction on test data\n",
    "df_preds_test = pd.DataFrame({'File':FileNames, 'Prediction':y_pred_test_classes})\n",
    "df_preds_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ground truth - StartingMaterial\n",
    "df_preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_test.merge(df_preds_test,on='File',how='left')\n",
    "result.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as CSV file\n",
    "result.to_csv(csv_result, index=False, na_rep = 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generete proper arrays\n",
    "List_TrueClass_test = result['Label'].tolist()\n",
    "List_PredClass_test = result['Prediction'].tolist()\n",
    "\n",
    "# Back to class_nb\n",
    "List_TrueValue_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_TrueClass_test]\n",
    "List_PredValue_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_PredClass_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(List_TrueClass_test[:10])\n",
    "print(List_PredClass_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(List_TrueValue_test[:10])\n",
    "print(List_PredValue_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "print(unique_labels(List_TrueValue_test))\n",
    "print(unique_labels(List_PredValue_test))\n",
    "print(unique_labels(List_TrueValue_test, List_PredValue_test))\n",
    "a = unique_labels(List_TrueValue_test, List_PredValue_test)\n",
    "print(a)\n",
    "b = [learn.data.classes[i] for i in a]\n",
    "print(b)\n",
    "#classes = learn.data.classes[unique_labels(List_TrueValue_test, List_PredValue_test)]\n",
    "#print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(List_PredValue_test,List_TrueValue_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(List_TrueClass_test,List_PredClass_test, labels=learn.data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    #classes = [classes[i] for i in unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    plt.grid(False,which='major')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_confusion_matrix(List_TrueValue_test, List_PredValue_test, learn.data.classes, title='Confusion Matrix - Test data')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ConfusionMatrix_TestData.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(List_TrueValue_test,List_PredValue_test,target_names=learn.data.classes,digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_TrueLabel_MajVoting = result.groupby(['Acquisition'])['Label'].apply(lambda x: x.mode()[0]).reset_index(name='TrueLabel_MajVoting')\n",
    "result_TrueLabel_MajVoting.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.groupby(['Acquisition'])['Prediction'].apply(lambda x: x.mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_PredLabel_MajVoting = result.groupby(['Acquisition'])['Prediction'].apply(lambda x: x.mode()[0]).reset_index(name='PredLabel_MajVoting')\n",
    "result_PredLabel_MajVoting.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data frames (to double check proper order)\n",
    "result_MajVoting = pd.merge(result_TrueLabel_MajVoting, result_PredLabel_MajVoting, how='left', on='Acquisition')\n",
    "result_MajVoting.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_MajVoting.to_csv(csv_result_MajVoting, index=False, na_rep = 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_MajVoting.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generete proper arrays\n",
    "List_TrueClass_MajVoting_test = result_MajVoting['TrueLabel_MajVoting'].tolist()\n",
    "List_PredClass_MajVoting_test = result_MajVoting['PredLabel_MajVoting'].tolist()\n",
    "\n",
    "# Back to class_nb\n",
    "List_TrueValue_MajVoting_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_TrueClass_MajVoting_test]\n",
    "List_PredValue_MajVoting_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_PredClass_MajVoting_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(List_PredValue_MajVoting_test,List_TrueValue_MajVoting_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(List_TrueClass_MajVoting_test,List_PredClass_MajVoting_test, labels=learn.data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(List_TrueValue_MajVoting_test,List_PredValue_MajVoting_test,target_names=learn.data.classes,digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(List_TrueClass_MajVoting_test,List_PredClass_MajVoting_test, labels=learn.data.classes)\n",
    "cm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_confusion_matrix(List_TrueValue_MajVoting_test, List_PredValue_MajVoting_test, \\\n",
    "                           learn.data.classes, title='Confusion Matrix - Test data - MajVoting')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ConfusionMatrix_TestData-MajVoting.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
