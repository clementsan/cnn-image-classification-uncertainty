{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  Generate confusion matrices on validation data from uncertainty assessment\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup CUDA_VISIBLE DEVICES for titan.sci.utah.edu\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries - fastai_v1\n",
    "\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from fastai.basic_train import DatasetType\n",
    "from fastai.torch_core import to_np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O and hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and hyper-parameters\n",
    "\n",
    "\n",
    "# CSV file contains validation dataset only (synthetic data)\n",
    "csv_val_FileName = 'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val.csv'\n",
    "csv_val = os.path.join('../CSV_InputFiles_TargetClass',csv_val_FileName)\n",
    "\n",
    "# CSV file contains validation dataset only (synthetic data)\n",
    "csv_result = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val-Prediction.csv')\n",
    "csv_result_Uncertainty = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val-PredictionWithUncertainty.csv')\n",
    "\n",
    "csv_result_MajVoting = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val-PredictionWithUncertainty_MajVoting.csv')\n",
    "#csv_result_Top2_MajVoting = os.path.join(os.getcwd(),'Dataset_MixedMaterials_ImageClassification_oversample_shuffled_fastai-v1_val-PredictionTop2_MajVoting.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file and create dataframe\n",
    "df_val = pd.read_csv(csv_val, sep=',')\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file and create dataframe\n",
    "df_preds_val_U = pd.read_csv(csv_result_Uncertainty, sep=',')\n",
    "df_preds_val_U.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_val_U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate ground truth - StartingMaterial\n",
    "# df_preds_val_U['Label'] = df_preds_val_U['File'].apply(lambda x: x.split('/', -1)[0])\n",
    "# df_preds_val_U.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_preds_val_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns_plot = sns.countplot(x=\"Label\", data=result)\n",
    "sns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=90)\n",
    "fig = sns_plot.get_figure()\n",
    "#fig.savefig(\"BarGraph_Distribution_StartingMaterial_Val.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    #classes = [classes[i] for i in unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    plt.grid(False,which='major')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(matrix, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        title = 'Average matrix'\n",
    "\n",
    "    #classes = [classes[i] for i in unique_labels(y_true, y_pred)]\n",
    "    print('Average matrix')\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,8))\n",
    "    \n",
    "    plt.grid(False)\n",
    "    im = ax.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(matrix.shape[1]),\n",
    "           yticks=np.arange(matrix.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    #fmt = '.2f' if normalize else 'd'\n",
    "    fmt ='.3f'\n",
    "    thresh = np.nanmax(matrix) / 1.5\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            ax.text(j, i, format(matrix[i, j], fmt), fontsize=10, \n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate matrices on Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_TrueClass_val = result['Label'].tolist()\n",
    "List_PredClass_val = result['Prediction'].tolist()\n",
    "List_Values_Entropy = result['Entropy'].tolist()\n",
    "List_Values_PredMean = result['Pred_Mean'].tolist()\n",
    "\n",
    "classes = ['Class1', \\\n",
    " 'Class2', \\\n",
    " 'Class3', \\\n",
    " 'Class4', \\\n",
    " 'Class5']\n",
    "\n",
    "# Back to class_nb\n",
    "List_TrueValue_val = [pd.Index(classes).get_loc(x) for x in List_TrueClass_val]\n",
    "List_PredValue_val = [pd.Index(classes).get_loc(x) for x in List_PredClass_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_List_TrueClass_val = List_TrueClass_val[:10]\n",
    "tmp_List_TrueValue_val = [pd.Index(classes).get_loc(x) for x in tmp_List_TrueClass_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp_List_TrueClass_val)\n",
    "print(tmp_List_TrueValue_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_confusion_matrix(List_TrueValue_val, List_PredValue_val, classes, title='Confusion Matrix - Validation data')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ConfusionMatrix-Uncertainty_ValData.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate average matrix (each cell display average value per predicted / true class)\n",
    "\n",
    "def generate_averagematrix(actual, predicted, values, labels):\n",
    "    cm = np.zeros((len(labels), len(labels)))\n",
    "    am = np.zeros((len(labels), len(labels)))\n",
    "    for a, p, v in zip(actual, predicted, values):\n",
    "        cm[a][p] += 1\n",
    "        am[a][p] += v\n",
    "    # Generate average value\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            am[i][j] = am[i][j] / cm[i][j]\n",
    "    return am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate matrices\n",
    "\n",
    "am_Entropy = generate_averagematrix(List_TrueValue_val, List_PredValue_val, List_Values_Entropy, classes)\n",
    "\n",
    "am_PredMean = generate_averagematrix(List_TrueValue_val, List_PredValue_val, List_Values_PredMean, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_matrix(am_Entropy, classes, title='Matrix - Average entropy')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matrix_AverageEntropy_ValData.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_matrix(am_PredMean, classes, title='Matrix - Average Pred')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matrix_AveragePredMean_ValData.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results - am_PredMean\n",
    "Nb_Diag = am_PredMean.shape[1]\n",
    "am_PredMean_Avg = np.nanmean(am_PredMean)\n",
    "am_PredMean_Avg_Trace = am_PredMean.trace() / Nb_Diag\n",
    "print(Nb_Diag,round(am_PredMean_Avg,4),round(am_PredMean_Avg_Trace,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results - am_Entropy\n",
    "Nb_Diag = am_Entropy.shape[1]\n",
    "am_Entropy_Avg = np.nanmean(am_Entropy)\n",
    "am_Entropy_Avg_Trace = am_Entropy.trace() / Nb_Diag\n",
    "print(Nb_Diag,round(am_Entropy_Avg,4),round(am_Entropy_Avg_Trace,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
