{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  TargetClass - 25000x -  inference on Validation data - uncertainty assessment\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup CUDA_VISIBLE DEVICES for titan.sci.utah.edu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries - fastai_v1\n",
    "\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from fastai.basic_train import DatasetType\n",
    "from fastai.torch_core import to_np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O and hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and hyper-parameters\n",
    "\n",
    "# CSV file contains test dataset only (synthetic data)\n",
    "csv_val_FileName = 'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val.csv'\n",
    "csv_val = os.path.join('../CSV_InputFiles_TargetClass',csv_val_FileName)\n",
    "\n",
    "csv_result = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val-Prediction.csv')\n",
    "csv_result_Uncertainty = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val-PredictionWithUncertainty.csv')\n",
    "\n",
    "csv_result_MajVoting = os.path.join(os.getcwd(),'Dataset_TargetClass_Overlap-9Blocks_25000xOnly_shuffled_fastai-v1_val-PredictionWithUncertainty_MajVoting.csv')\n",
    "#csv_result_Top2_MajVoting = os.path.join(os.getcwd(),'Dataset_MixedMaterials_ImageClassification_oversample_shuffled_fastai-v1_val-PredictionTop2_MajVoting.csv')\n",
    "\n",
    "# Network\n",
    "model_path = os.path.join(os.getcwd(),'models')\n",
    "model_file = ('TargetClass_fastai-v1_224_all_resnet50.pkl')\n",
    "\n",
    "# Network architecture\n",
    "arch = models.resnet50\n",
    "# Image size\n",
    "sz = 224\n",
    "# Batch size\n",
    "bs = 32\n",
    "# Default learning rate\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file and create dataframe\n",
    "df_val = pd.read_csv(csv_val, sep=',')\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.groupby(['Label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns_plot = sns.countplot(x=\"Label\", data=df_val)\n",
    "sns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig = sns_plot.get_figure()\n",
    "fig.savefig(\"BarGraph_Distribution_Label_ValData.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CNN Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate validation ImageList\n",
    "val_ImageList = ImageList.from_csv(os.getcwd(), csv_val_FileName, folder='../Data_TargetClass')\n",
    "\n",
    "print(val_ImageList)\n",
    "print(type(val_ImageList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CNN model along with wegihts, transforms, and test data)\n",
    "learn = load_learner(model_path,model_file, test=val_ImageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: incorrect label, as loaded as DatasetType.Test\n",
    "#learn.data.show_batch(rows=3, ds_type=DatasetType.Test, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get direct predictions\n",
    "y_pred_test, _, losses = learn.get_preds(ds_type=DatasetType.Test,with_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classes from predictions\n",
    "y_pred_test_classes = [learn.data.classes[np.argmax(pred)] for pred in y_pred_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.test_ds.items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileNames = ['/'.join(i.split('/', -1)[-4:]) for i in learn.data.test_ds.items]\n",
    "df_preds_test = pd.DataFrame({'File':FileNames, 'Prediction':y_pred_test_classes})\n",
    "df_preds_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new dataframe result\n",
    "result = df_val.merge(df_preds_test,on='File',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as CSV file\n",
    "result.to_csv(csv_result, index=False, na_rep = 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and confusion matrix\n",
    "\n",
    "\n",
    "# Generate proper arrays\n",
    "List_TrueClass_test = result['Label'].tolist()\n",
    "List_PredClass_test = result['Prediction'].tolist()\n",
    "\n",
    "# Back to class_nb\n",
    "List_TrueValue_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_TrueClass_test]\n",
    "List_PredValue_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_PredClass_test]\n",
    "\n",
    "# Accuracy score\n",
    "print(\"\\nAccuracy score:\", accuracy_score(List_TrueValue_test, List_PredValue_test))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(List_TrueValue_test,List_PredValue_test,target_names=learn.data.classes))\n",
    "\n",
    "\n",
    "#Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(List_TrueClass_test,List_PredClass_test, labels=learn.data.classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    #classes = [classes[i] for i in unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    plt.grid(False,which='major')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_confusion_matrix(List_TrueValue_test, List_PredValue_test, learn.data.classes, title='Confusion Matrix - Val data')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ConfusionMatrix_ValData.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functions to update model, measure entropy, plot histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    \"\"\"Custom Dropout module to be used as a baseline for MC Dropout\"\"\"\n",
    "\n",
    "    def __init__(self, p:float, activate=True):\n",
    "        super().__init__()\n",
    "        self.activate = activate\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.dropout(x, self.p, training=self.training or self.activate)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"p={self.p}, activate={self.activate}\"\n",
    "\n",
    "\n",
    "def switch_custom_dropout(m, activate:bool=True, verbose:bool=False):\n",
    "    \"\"\"Turn all Custom Dropouts training mode to true or false according to the variable activate\"\"\"\n",
    "    for c in m.children():\n",
    "        if isinstance(c, CustomDropout):\n",
    "            print(f\"Current active : {c.activate}\")\n",
    "            print(f\"Switching to : {activate}\")\n",
    "            c.activate = activate\n",
    "        else:\n",
    "            switch_custom_dropout(c, activate=activate)\n",
    "\n",
    "def convert_layers(model:nn.Module, original:nn.Module, replacement:nn.Module, get_args:Callable=None,\n",
    " additional_args:dict={}):\n",
    "    \"\"\"Convert modules of type \"original\" to \"replacement\" inside the model\n",
    "    \n",
    "    get_args : a function to use on the original module to eventually get its arguements to pass to the new module\n",
    "    additional_args : a dictionary to add more args to the new module\n",
    "    \"\"\"\n",
    "    for child_name, child in model.named_children():\n",
    "\n",
    "        if isinstance(child, original):\n",
    "            # First we grab args from the child\n",
    "            if get_args:\n",
    "                original_args = get_args(child)\n",
    "            else:\n",
    "                original_args = {}\n",
    "\n",
    "            # If we want to provide additional args\n",
    "            if additional_args:\n",
    "                args = {**original_args, **additional_args}\n",
    "            else:\n",
    "                args = original_args\n",
    "\n",
    "            new_layer = replacement(**args)\n",
    "            setattr(model, child_name, new_layer)\n",
    "        else:\n",
    "            convert_layers(child, original, replacement,\n",
    "                           get_args, additional_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nb_classes = len(learn.data.classes)\n",
    "\n",
    "# Note: updated entropy metric with normalization by number of classes\n",
    "def entropy(probs, softmax=False):\n",
    "    \"\"\"Return the prediction of a T*N*C tensor with :\n",
    "        - T : the number of samples\n",
    "        - N : the batch size\n",
    "        - C : the number of classes\n",
    "    \"\"\"\n",
    "    probs = to_np(probs)\n",
    "    prob = probs.mean(axis=0)\n",
    "\n",
    "    entrop = - (np.log(prob) * prob).sum(axis=1)\n",
    "    entrop = entrop / nb_classes\n",
    "    return entrop\n",
    "\n",
    "\n",
    "def uncertainty_best_probability(probs):\n",
    "    \"\"\"Return standard deviation of the most probable class\"\"\"\n",
    "    idx = probs.mean(axis=0).argmax(axis=1)\n",
    "\n",
    "    std = probs[:, np.arange(len(idx)), idx].std(axis=0)\n",
    "\n",
    "    return std\n",
    "\n",
    "\n",
    "def uncertainty_best_probability2(probs):\n",
    "    \"\"\"Return idx, mean and standard deviation of the most probable class\"\"\"\n",
    "    idx = probs.mean(axis=0).argmax(axis=1)\n",
    "    probs_idx = probs[:, np.arange(len(idx)), idx]\n",
    "    mean = probs_idx.mean(axis=0)\n",
    "    std = probs_idx.std(axis=0)\n",
    "    return idx, mean, std\n",
    "\n",
    "\n",
    "def BALD(probs):\n",
    "    \"\"\"Information Gain, distance between the entropy of averages and average of entropy\"\"\"\n",
    "    entrop1 = entropy(probs)\n",
    "    probs = to_np(probs)\n",
    "\n",
    "    entrop2 = - (np.log(probs) * probs).sum(axis=2)\n",
    "    entrop2 = entrop2.mean(axis=0)\n",
    "    entrop2 = entrop2 / nb_classes\n",
    "    \n",
    "    ig = entrop1 - entrop2\n",
    "    return ig\n",
    "\n",
    "\n",
    "def top_k_uncertainty(s, k=5, reverse=True):\n",
    "    \"\"\"Return the top k indexes\"\"\"\n",
    "    sorted_s = sorted(list(zip(np.arange(len(s)), s)),\n",
    "                      key=lambda x: x[1], reverse=reverse)\n",
    "    output = [sorted_s[i][0] for i in range(k)]\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_preds_sample(learn, ds_type=DatasetType.Test, n_sample=10, reduce=None,activ=None,with_loss=False):\n",
    "    \"\"\"Get MC Dropout predictions from a learner, and eventually reduce the samples\"\"\"\n",
    "    preds = []\n",
    "    for i in range(n_sample):\n",
    "        pred, y = learn.get_preds(ds_type=ds_type, activ=activ)\n",
    "        pred = pred.view((1,) + pred.shape)\n",
    "        preds.append(pred)\n",
    "    preds = torch.cat(preds)\n",
    "    if reduce == \"mean\":\n",
    "        preds = preds.mean(dim=0)\n",
    "    elif reduce == \"std\":\n",
    "        preds = preds.std(dim=0)\n",
    "    return preds, y\n",
    "\n",
    "\n",
    "# Warning: only correct for binary classification? need to double-check implementation\n",
    "def plot_hist_groups(pred,y,metric,bins=None,figsize=(16,16)):\n",
    "    TP = to_np((pred.mean(dim=0).argmax(dim=1) == y) & (y == 1))\n",
    "    TN = to_np((pred.mean(dim=0).argmax(dim=1) == y) & (y == 0))\n",
    "    FP = to_np((pred.mean(dim=0).argmax(dim=1) != y) & (y == 0))\n",
    "    FN = to_np((pred.mean(dim=0).argmax(dim=1) != y) & (y == 1))\n",
    "    \n",
    "    result = metric(pred)\n",
    "    \n",
    "    TP_result = result[TP]\n",
    "    TN_result = result[TN]\n",
    "    FP_result = result[FP]\n",
    "    FN_result = result[FN]\n",
    "    \n",
    "    fig,ax = plt.subplots(2,2,figsize=figsize)\n",
    "    \n",
    "    sns.distplot(TP_result,ax=ax[0,0],bins=bins)\n",
    "    ax[0,0].set_title(f\"True positive\")\n",
    "    \n",
    "    sns.distplot(TN_result,ax=ax[0,1],bins=bins)\n",
    "    ax[0,1].set_title(f\"True negative\")\n",
    "    \n",
    "    sns.distplot(FP_result,ax=ax[1,0],bins=bins)\n",
    "    ax[1,0].set_title(f\"False positive\")\n",
    "    \n",
    "    sns.distplot(FN_result,ax=ax[1,1],bins=bins)\n",
    "    ax[1,1].set_title(f\"False negative\")\n",
    "    \n",
    "    \n",
    "def predict_entropy(img,n_times=10):\n",
    "    pred = learn.predict_with_mc_dropout(img,n_times=n_times)\n",
    "    probs = [prob[2].view((1,1) + prob[2].shape) for prob in pred]\n",
    "    probs = torch.cat(probs)\n",
    "    e = entropy(probs)\n",
    "    return e\n",
    "\n",
    "def plot_img_with_entropy(img,n_times=10):\n",
    "    e = predict_entropy(img,n_times=n_times)\n",
    "    img = to_np(img.data.permute(1,2,0))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(b=None)\n",
    "    plt.title(\"Entropy %f:\" %(e[0]))\n",
    "    \n",
    "def predict_uncertainty(img,n_times=10):\n",
    "    pred = learn.predict_with_mc_dropout(img,n_times=n_times)\n",
    "    probs = [prob[2].view((1,1) + prob[2].shape) for prob in pred]\n",
    "    probs = torch.cat(probs)\n",
    "    idx, mean, std = uncertainty_best_probability2(probs)\n",
    "    return idx, mean, std\n",
    "\n",
    "def plot_img_with_uncertainty(img,n_times=10):\n",
    "    idx, mean, std = predict_uncertainty(img,n_times=n_times)\n",
    "    img = to_np(img.data.permute(1,2,0))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(b=None)\n",
    "    plt.title(\"Inference with uncertainty (label,mean,std): \\n %i, %f +- %f \" % (idx,mean,std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate sample predictions\n",
    "# preds_sample,y_sample = get_preds_sample(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate list of ground truth labels from dataframe\n",
    "# List_StartingMaterials_Val = df_val['StartingMaterial'].tolist()\n",
    "# y_sample_List = [learn.data.classes.index(label) for label in List_StartingMaterials_Val]\n",
    "# y_sample = torch.FloatTensor(y_sample_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_hist_groups(preds_sample,y_sample,entropy,bins=20,figsize=(14,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments - uncertainty on single test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = learn.data.test_ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_U = learn.predict_with_mc_dropout(img,n_times=100)\n",
    "pred_U[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [prob[2].view((1,1) + prob[2].shape) for prob in pred_U]\n",
    "probs = torch.cat(probs)\n",
    "print(probs.shape)\n",
    "#probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: uncertainty enabled by default code\n",
    "plot_img_with_entropy(img,n_times=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: uncertainty enabled by default code\n",
    "plot_img_with_uncertainty(img,n_times=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: uncertainty enabled by default code\n",
    "plot_img_with_uncertainty(img,n_times=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nn.Dropout to CustomDropout module\n",
    "get_args = lambda dp : {\"p\" : dp.p}\n",
    "convert_layers(learn.model,nn.Dropout,CustomDropout,get_args)\n",
    "\n",
    "# Turn on the stochasticity, I use verbose just to make sure it's working fine\n",
    "switch_custom_dropout(learn.model,True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with uncertainty -  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample predictions\n",
    "preds_sample_U, _ = get_preds_sample(learn, n_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sample_U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return list of metrics for test dataset: mean, std, entropy\n",
    "def uncertainty_metrics_sample():\n",
    "    List_Predidx = []\n",
    "    List_Predmean = []\n",
    "    List_Predstd = []\n",
    "    List_entropy = []\n",
    "    for i in range(len(learn.data.test_ds)):\n",
    "            probs = preds_sample_U[:,i,:]\n",
    "            probs = probs.unsqueeze(1)\n",
    "            Predidx, Predmean, Predstd = uncertainty_best_probability2(probs)\n",
    "            e = entropy(probs)\n",
    "    \n",
    "            List_Predidx.append(Predidx.numpy()[0])\n",
    "            List_Predmean.append(Predmean.numpy()[0])\n",
    "            List_Predstd.append(Predstd.numpy()[0])\n",
    "            List_entropy.append(e[0])\n",
    "\n",
    "    return List_Predidx, List_Predmean, List_Predstd, List_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Predidx, List_Predmean, List_Predstd, List_entropy = uncertainty_metrics_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(List_Predidx[:10])\n",
    "print(List_Predmean[:10])\n",
    "print(List_Predstd[:10])\n",
    "print(List_entropy[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classes from predictions\n",
    "y_pred_test_classes_U = [learn.data.classes[idx] for idx in List_Predidx]\n",
    "print(y_pred_test_classes_U[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframe and save spreadsheet from all samples\n",
    "\n",
    "FileNames = ['/'.join(i.split('/', -1)[-4:]) for i in learn.data.test_ds.items]\n",
    "\n",
    "df_preds_test_U = pd.DataFrame({'File':FileNames, 'Prediction':y_pred_test_classes_U,'Pred_Mean':List_Predmean, 'Pred_Std':List_Predstd, 'Entropy':List_entropy})\n",
    "df_preds_test_U.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data frames (to double check proper order)\n",
    "result_U = df_val.merge(df_preds_test_U,on='File',how='left')\n",
    "df_preds_test_U.shape, result_U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_U.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save spreadsheet from uncertainty\n",
    "result_U.to_csv(csv_result_Uncertainty, index=False, na_rep = 'NA',float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    #classes = [classes[i] for i in unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    plt.grid(False,which='major')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and confusion matrix\n",
    "\n",
    "\n",
    "# Generate proper arrays\n",
    "List_TrueClass_test_U = result_U['Label'].tolist()\n",
    "List_PredClass_test_U = result_U['Prediction'].tolist()\n",
    "\n",
    "# Back to class_nb\n",
    "List_TrueValue_test_U = [pd.Index(learn.data.classes).get_loc(x) for x in List_TrueClass_test_U]\n",
    "List_PredValue_test_U = [pd.Index(learn.data.classes).get_loc(x) for x in List_PredClass_test_U]\n",
    "\n",
    "# Accuracy score\n",
    "print(\"\\nAccuracy score:\", accuracy_score(List_PredValue_test_U,List_TrueValue_test_U))\n",
    "\n",
    "#Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(List_TrueClass_test_U,List_PredClass_test_U, labels=learn.data.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_confusion_matrix(List_TrueValue_test_U, List_PredValue_test_U, learn.data.classes, title='Confusion Matrix - Val data - Uncertainty')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ConfusionMatrix_ValData-Uncertainty.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_U_StartingMaterial_mean = result_U.groupby('Label')['Pred_Mean','Pred_Std','Entropy'].mean()\n",
    "#result_U_StartingMaterial_mean = result_U_StartingMaterial_mean.reindex(classes_Labels_ordered)\n",
    "result_U_StartingMaterial_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_U_StartingMaterial_std = result_U.groupby('Label')['Pred_Mean','Pred_Std','Entropy'].std()\n",
    "#result_U_StartingMaterial_std = result_U_StartingMaterial_std.reindex(classes_Labels_ordered)\n",
    "result_U_StartingMaterial_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_U.groupby('Label').describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot histograms of entropy for each starting material\n",
    "\n",
    "# def plot_hist_metric(df,metric,bins=10,figsize=(16,16)):\n",
    "    \n",
    "#     List_25ADU75UO4 = df[df['StartingMaterial']=='25ADU-75UO4'][metric].tolist()\n",
    "#     List_50ADU50UO4 = df[df['StartingMaterial']=='50ADU-50UO4'][metric].tolist()\n",
    "#     List_75ADU25UO4 = df[df['StartingMaterial']=='75ADU-25UO4'][metric].tolist()\n",
    "#     List_ADU = df[df['StartingMaterial']=='ADU'][metric].tolist()\n",
    "#     List_UO4 = df[df['StartingMaterial']=='UO4-2H2O'][metric].tolist()\n",
    "    \n",
    "    \n",
    "#     fig,ax = plt.subplots(3,2,figsize=figsize)\n",
    "#     #plt.xlim(0, 0.25)\n",
    "    \n",
    "#     #sns.distplot(List_25ADU75UO4,ax=ax[0,0],bins=bins,hist_kws={'range': (0.0, 0.25)},kde_kws={'clip': (0.0, 0.25)})\n",
    "#     sns.distplot(List_25ADU75UO4,ax=ax[0,0],bins=bins)\n",
    "#     ax[0,0].set_title(f\"List_25ADU75UO4\")\n",
    "#     ax[0,0].set_xlim([-0.05, 0.25])\n",
    "    \n",
    "#     sns.distplot(List_50ADU50UO4,ax=ax[0,1],bins=bins)\n",
    "#     ax[0,1].set_title(f\"List_50ADU50UO4\")\n",
    "#     ax[0,1].set_xlim([-0.05, 0.25])\n",
    "    \n",
    "#     sns.distplot(List_75ADU25UO4,ax=ax[1,0],bins=bins)\n",
    "#     ax[1,0].set_title(f\"List_75ADU25UO4\")\n",
    "#     ax[1,0].set_xlim([-0.05, 0.25])\n",
    "    \n",
    "#     sns.distplot(List_ADU,ax=ax[1,1],bins=bins)\n",
    "#     ax[1,1].set_title(f\"List_ADU\")\n",
    "#     ax[1,1].set_xlim([-0.05, 0.25])\n",
    "    \n",
    "#     sns.distplot(List_UO4,ax=ax[2,0],bins=bins,hist_kws={'range': (0.0, 0.25)},kde_kws={'clip': (0.0, 0.25)})\n",
    "#     ax[2,0].set_title(f\"List_UO4\")\n",
    "#     ax[2,0].set_xlim([-0.05, 0.25])\n",
    "    \n",
    "    \n",
    "#     # Save the full figure...\n",
    "#     #fig.savefig('Histogram_Entropy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_hist_metric(result_U,\"Entropy\")\n",
    "# plt.savefig(\"Screenshot_Histograms_Label-ValData_Entropy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random image to assess larger uncertainty..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_TrueLabel_MajVoting = result.groupby(['Acquisition'])['StartingMaterial'].apply(lambda x: x.mode()[0]).reset_index(name='TrueLabel_MajVoting')\n",
    "result_TrueLabel_MajVoting.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.groupby(['Acquisition'])['Prediction'].apply(lambda x: x.mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_PredLabel_MajVoting = result.groupby(['Acquisition'])['Prediction'].apply(lambda x: x.mode()[0]).reset_index(name='PredLabel_MajVoting')\n",
    "result_PredLabel_MajVoting.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data frames (to double check proper order)\n",
    "result_MajVoting = pd.merge(result_TrueLabel_MajVoting, result_PredLabel_MajVoting, how='left', on='Acquisition')\n",
    "result_MajVoting.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_MajVoting.to_csv(csv_result_MajVoting, index=False, na_rep = 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_MajVoting.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generete proper arrays\n",
    "List_TrueClass_MajVoting_test = result_MajVoting['TrueLabel_MajVoting'].tolist()\n",
    "List_PredClass_MajVoting_test = result_MajVoting['PredLabel_MajVoting'].tolist()\n",
    "\n",
    "# Back to class_nb\n",
    "List_TrueValue_MajVoting_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_TrueClass_MajVoting_test]\n",
    "List_PredValue_MajVoting_test = [pd.Index(learn.data.classes).get_loc(x) for x in List_PredClass_MajVoting_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(List_PredValue_MajVoting_test,List_TrueValue_MajVoting_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(List_TrueClass_MajVoting_test,List_PredClass_MajVoting_test, labels=learn.data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(List_TrueValue_MajVoting_test,List_PredValue_MajVoting_test,target_names=learn.data.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
